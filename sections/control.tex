Quantum systems require precise control to maintain coherence and execute operations with high fidelity. AI techniques offer significant advantages for both offline control optimization and real-time error management.

\subsection{Optimal Control Theory}

Quantum optimal control seeks time-dependent control fields $\{c_j(t)\}$ that drive a quantum system governed by the Hamiltonian:

\begin{equation}
H(t) = H_0 + \sum_j c_j(t) H_j
\end{equation}

where $H_0$ is the drift Hamiltonian and $\{H_j\}$ are control Hamiltonians. The goal is to evolve the system from an initial state $\rho_0$ to a target state $\rho_T$ or to implement a target unitary $U_T$.

Traditional approaches like GRAPE (Gradient Ascent Pulse Engineering) use gradient information to optimize control parameters. AI methods extend these capabilities through model-free optimization techniques. Reinforcement learning agents can discover high-fidelity control sequences without explicit system models \cite{bukov2018reinforcement}, enabling control optimization even when accurate system models are unavailable or prohibitively complex.

Neural network controllers exhibit remarkable robustness to uncertainty in system parameters. By incorporating parameter variations during training, these controllers learn policies that maintain performance despite fabrication imperfections or environmental fluctuations. This robustness is particularly valuable for scaling quantum systems, where consistent performance across multiple qubits with varying parameters presents a significant challenge.

One of the most compelling demonstrations of AI-based quantum control is the discovery of protocols that exceed the speed limits of adiabatic control \cite{ding2021breaking}. While conventional adiabatic approaches require slow evolution to avoid excitations to unwanted states, RL agents have identified non-adiabatic protocols that achieve the same final states in significantly less time. These counter-intuitive control strategies can enable more operations within coherence time constraints, addressing a fundamental limitation of quantum computing systems.

\subsection{Real-time Adaptive Control}

Real-time control adjusts operations based on continuous measurement feedback. For a partially observable quantum system, the control problem becomes a partially observable Markov decision process (POMDP) where states represent quantum density matrices $\rho$, actions are control operations, and observations are measurement outcomes with probability distributions determined by quantum mechanics.

Deep reinforcement learning strategies for this POMDP can be formulated using recurrent neural networks that maintain internal representations of quantum state estimates. The resulting controllers can generate control policies conditioned on measurement history:

\begin{equation}
\pi(a_t | o_1, o_2, \ldots, o_t) = \text{RNN}(o_t, h_{t-1})
\end{equation}

where $o_t$ are observations, $a_t$ are actions, and $h_t$ is the hidden state of the RNN.

Adaptive control has demonstrated particular value for non-Markovian quantum systems, where environmental memory effects create temporal correlations in system dynamics. Recurrent neural networks can capture these temporal correlations and adjust control strategies accordingly, outperforming memoryless controllers. For example, RNN controllers have been applied to superconducting qubits subject to $1/f$ noise, adaptively compensating for low-frequency drift that would otherwise cause gate errors to accumulate.

Another promising application is measurement-based feedback for quantum error correction. Neural network policies can determine optimal correction operations based on syndrome measurement sequences, accounting for both the observed error syndromes and the statistical correlations between errors. These adaptive approaches demonstrate higher error correction thresholds than standard table-based decoders, particularly for complex noise models with temporal and spatial correlations.

\subsection{Multi-qubit Operation Scheduling}

Executing multiple operations across a quantum processor requires sophisticated scheduling to minimize crosstalk and maximize parallelism. Machine learning approaches formulate this as a constrained optimization problem:

\begin{equation}
\min_{\{t_i\}} \sum_i w_i t_i \quad \text{subject to constraints on overlapping operations}
\end{equation}

where $t_i$ represents the start time of operation $i$ and $w_i$ its priority weight.

AI schedulers consider hardware-specific constraints such as control line bandwidth limitations, which restrict simultaneous operations on qubits sharing control infrastructure. They also account for crosstalk between neighboring qubits, which can cause unintended interactions when gates are executed in parallel. By learning from simulation and experimental data, these schedulers develop sophisticated models of inter-qubit interference and optimize operation timing to minimize these effects.

Time-dependent noise characteristics present another scheduling challenge that machine learning approaches effectively address. Many quantum systems exhibit periodic noise fluctuations due to environmental factors or control system limitations. AI schedulers learn these temporal patterns and preferentially schedule high-precision operations during low-noise windows, significantly improving overall computational fidelity.

Variability in operation fidelities across different qubits and gate types also influences optimal scheduling decisions. Reinforcement learning agents learn to prioritize more reliable operations and allocate more precise qubits to critical computational paths, maximizing the probability of successful algorithm execution. This dynamic resource allocation improves computational results without requiring hardware modifications.

\subsection{In-situ Calibration and Drift Compensation}

Quantum systems exhibit parameter drift over time, requiring continuous recalibration. AI techniques enable efficient in-situ calibration through Bayesian experimental design, which selects optimal calibration experiments to maximize information gain. By modeling the uncertainty in system parameters and choosing measurements that most effectively reduce this uncertainty, Bayesian approaches minimize the time spent on calibration procedures.

Online learning approaches continuously update system models based on observed performance during computation. Rather than treating calibration as a separate procedure, these techniques integrate parameter estimation into regular operation, detecting and compensating for drift in real-time. For example, Kalman filter-based approaches have been implemented to track qubit frequency drift in superconducting systems, applying compensating fields without interrupting computation.

Predictive maintenance represents another valuable application of AI for quantum control systems. By analyzing temporal patterns in system parameters, machine learning models can forecast imminent parameter drift and schedule preventive recalibration before performance degrades significantly. These predictive approaches minimize unexpected computational failures and optimize the allocation of calibration time.

These control techniques significantly enhance quantum computer performance by optimizing hardware operation at multiple timescales - from nanosecond pulse shaping to millisecond measurement feedback and hour-to-hour drift compensation. The integration of machine learning throughout the control stack enables more efficient utilization of quantum coherence, ultimately improving computational capabilities. 